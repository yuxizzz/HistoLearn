% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate_model.R
\name{evaluate_model}
\alias{evaluate_model}
\title{Evaluate Model Performance}
\usage{
evaluate_model(trained_model, test_data)
}
\arguments{
\item{trained_model}{A fitted model object returned by \code{\link{train_model}}.
Must include components \code{dr_model}, \code{model}, and \code{dr_dim}.}

\item{test_data}{An object of class \code{"histofeature"} created by
\code{\link{load_embeddings}}, containing test features and labels.}
}
\value{
A list containing:
\itemize{
\item \code{conf_matrix} — a \code{ggplot} object showing the confusion matrix.
\item \code{metric} — a list with evaluation metrics (currently accuracy).
}
}
\description{
Evaluate a trained classification model on a test dataset. The function applies
the model’s dimensionality-reduction transformation to the test features,
generates predictions, computes overall accuracy, and visualizes the confusion
matrix using \pkg{ggplot2}.
}
\examples{
data(train_embeddings)
data(train_labels)
data(test_embeddings)
data(test_labels)
train_set <- load_embeddings(feature=train_embeddings, label=train_labels)
test_set <- load_embeddings(feature=test_embeddings, label=test_labels)
model <- train_model(feature_embedding=train_set,
                      dr="pca", dr_k=20, model = "knn")
evaluate_model(model, test_set)

}
\references{
Kuhn, M. (2008). Building Predictive Models in R Using the caret
Package. Journal of Statistical Software, 28(5), 1–26.
https://doi.org/10.18637/jss.v028.i05

H. Wickham. ggplot2: Elegant Graphics for Data Analysis.
Springer-Verlag New York, 2016.
}
